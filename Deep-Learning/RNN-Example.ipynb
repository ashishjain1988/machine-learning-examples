{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as ts\n",
    "import keras as ks\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "import scipy as sc\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(\"C:\\\\Users\\\\ctg3039\\\\Downloads\\\\FolgerDigitalTexts_TXT_Complete\\\\\", \"1H4.txt\")\n",
    "sample = open(train_path, \"r\") \n",
    "s = sample.read()\n",
    "data = [] \n",
    "#text = \"\\n\".join([ll.rstrip() for ll in s.splitlines() if ll.strip()])\n",
    "f = s.replace(\"\\n\", \" \")\n",
    "#Removing the non ASCII characters\n",
    "#f = f[np.where(f < 128)]\n",
    "# iterate through each sentence in the file \n",
    "for i in sent_tokenize(f):\n",
    "    #print(i)\n",
    "    temp = [] \n",
    "    # tokenize the sentence into words \n",
    "    for j in word_tokenize(i.strip()): \n",
    "        data.append(j.lower())\n",
    "    #data.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31675\n",
      "3886\n"
     ]
    }
   ],
   "source": [
    "print(len((data)))\n",
    "print(len(set(data)))\n",
    "#print(len(sentences_test))\n",
    "#print((sentences[2:4]))\n",
    "#print((next_words[2:4]))\n",
    "#print(len(next_words_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words before ignoring: 3886\n",
      "Ignoring words with frequency < 2\n",
      "Unique words after ignoring: 1650\n",
      "Ignored sequences: 15849\n",
      "Remaining sequences: 15816\n"
     ]
    }
   ],
   "source": [
    "# Calculate word frequency\n",
    "MIN_WORD_FREQUENCY = 2\n",
    "word_freq = {}\n",
    "for word in data:\n",
    "    word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "ignored_words = set()\n",
    "for k, v in word_freq.items():\n",
    "    if word_freq[k] < MIN_WORD_FREQUENCY:\n",
    "        ignored_words.add(k)\n",
    "\n",
    "words = set(data)\n",
    "print('Unique words before ignoring:', len(words))\n",
    "print('Ignoring words with frequency <', MIN_WORD_FREQUENCY)\n",
    "words = sorted(set(words) - ignored_words)\n",
    "print('Unique words after ignoring:', len(words))\n",
    "\n",
    "word_indices = dict((c, i) for i, c in enumerate(words))\n",
    "indices_word = dict((i, c) for i, c in enumerate(words))\n",
    "\n",
    "#Creating the sequences for the sentences\n",
    "STEP = 1\n",
    "SEQUENCE_LEN = 10\n",
    "sentences = []\n",
    "next_words = []\n",
    "ignored = 0\n",
    "for i in range(0, len(data) - SEQUENCE_LEN, STEP):\n",
    "    # Only add sequences where no word is in ignored_words\n",
    "    if len(set(data[i:i+SEQUENCE_LEN+1]).intersection(ignored_words)) == 0:\n",
    "        sentences.append(data[i: i + SEQUENCE_LEN])\n",
    "        next_words.append(data[i + SEQUENCE_LEN])\n",
    "    else:\n",
    "        ignored = ignored+1\n",
    "print('Ignored sequences:', ignored)\n",
    "print('Remaining sequences:', len(sentences))\n",
    "#sentences, next_words, sentences_test, next_words_test = sk.model_selection.train_test_split(sentences, next_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = np.zeros((len(sentences), SEQUENCE_LEN, len(words)), dtype=np.bool)\n",
    "#y = np.zeros((len(sentences), len(words)), dtype=np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(sentence_list, next_word_list, batch_size):\n",
    "    index = 0\n",
    "    while True:\n",
    "        x = np.zeros((batch_size, SEQUENCE_LEN, len(words)), dtype=np.bool)\n",
    "        y = np.zeros((batch_size, len(words)), dtype=np.bool)\n",
    "        for i in range(batch_size):\n",
    "            for t, w in enumerate(sentence_list[index]):\n",
    "                x[i, t, word_indices[w]] = 1\n",
    "            y[i, word_indices[next_word_list[index]]] = 1\n",
    "\n",
    "            index = index + 1\n",
    "            if index == len(sentence_list):\n",
    "                index = 0\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ctg3039\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\ctg3039\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(ks.layers.Bidirectional(LSTM(128), input_shape=(SEQUENCE_LEN, len(words))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(words)))\n",
    "model.add(ks.layers.Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "80/80 [==============================] - 38s 477ms/step - loss: 5.0766 - acc: 0.1141\n",
      "\n",
      "Epoch 00001: loss improved from inf to 5.07656, saving model to words-improvement-10-2.9530.hdf5\n",
      "Epoch 2/5\n",
      "80/80 [==============================] - 34s 423ms/step - loss: 4.9600 - acc: 0.1219\n",
      "\n",
      "Epoch 00002: loss improved from 5.07656 to 4.96001, saving model to words-improvement-10-2.9530.hdf5\n",
      "Epoch 3/5\n",
      "80/80 [==============================] - 33s 417ms/step - loss: 4.8688 - acc: 0.1321\n",
      "\n",
      "Epoch 00003: loss improved from 4.96001 to 4.86880, saving model to words-improvement-10-2.9530.hdf5\n",
      "Epoch 4/5\n",
      "80/80 [==============================] - 34s 430ms/step - loss: 4.8023 - acc: 0.1389\n",
      "\n",
      "Epoch 00004: loss improved from 4.86880 to 4.80234, saving model to words-improvement-10-2.9530.hdf5\n",
      "Epoch 5/5\n",
      "80/80 [==============================] - 37s 461ms/step - loss: 4.7313 - acc: 0.1449\n",
      "\n",
      "Epoch 00005: loss improved from 4.80234 to 4.73131, saving model to words-improvement-10-2.9530.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c7a5ddab38>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 200\n",
    "filename = \"words-improvement-10-2.9530.hdf5\"\n",
    "\n",
    "checkpoint = ks.callbacks.ModelCheckpoint(filename, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "#print_callback = ks.callbacks.LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "early_stopping = ks.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "callbacks_list = [checkpoint,early_stopping]\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit_generator(generator(sentences, next_words, BATCH_SIZE),\n",
    "    steps_per_epoch=int(len(sentences)/BATCH_SIZE) + 1,\n",
    "    epochs=5,\n",
    "    callbacks=callbacks_list)#,\n",
    "    #validation_data=generator(sentences_test, next_words_test, BATCH_SIZE),validation_steps=int(len(sentences_test)/BATCH_SIZE) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "model = Sequential()\n",
    "model.add(ks.layers.Bidirectional(LSTM(128), input_shape=(SEQUENCE_LEN, len(words))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(words)))\n",
    "model.add(ks.layers.Activation('softmax'))\n",
    "filename = \"words-improvement-10-2.9530.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8848\n",
      "Seed:\n",
      "\" faith.itellyouwhat--heheldme \"\n",
      ",iamalord,andiloweramiamaking.i'llnothim,iamalord,andiamaking.andi'llnotalord,andi'llbeafather.andi'llnotnotalord.i'llnotalord,andi'llnotafather.andi'llnotalord.andi'llnotnotalord.i'llnotalord,andi'llnotafather.andi'llnotalord\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "import sys\n",
    "start = np.random.randint(0, len(sentences)-1)\n",
    "print(start)\n",
    "pattern = np.zeros((1, SEQUENCE_LEN, len(words)), dtype=np.bool)\n",
    "for t, w in enumerate(sentences[start]):\n",
    "    pattern[0,t, word_indices[w]] = 1\n",
    "\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join(value for value in sentences[start]),\"\\\"\")\n",
    "# generate characters\n",
    "for i in range(100):\n",
    "    prediction = model.predict(pattern, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = indices_word[index]\n",
    "    sys.stdout.write(result)\n",
    "    t = np.zeros((len(words)), dtype=np.bool)\n",
    "    t[np.argmax(prediction)] = 1\n",
    "    height = pattern[0].shape[0]\n",
    "    pattern[0] = np.vstack([pattern[0], t])[1:(height+1),:]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 10, 1650)\n"
     ]
    }
   ],
   "source": [
    "r = generator(sentences, next_words, 200)\n",
    "print((next(r)[0].shape))\n",
    "#print(x.shape)\n",
    "#print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1221\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'lord'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = np.random.randint(0, len(sentences)-1)\n",
    "print(start)\n",
    "pattern = np.zeros((1, SEQUENCE_LEN, len(words)), dtype=np.bool)\n",
    "for t, w in enumerate(sentences[start]):\n",
    "    pattern[0,t, word_indices[w]] = 1\n",
    "prediction = model.predict(pattern, verbose=0)\n",
    "indices_word[np.argmax(prediction)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10, 1650)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ..., False, False, False])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.zeros((len(words)), dtype=np.bool)\n",
    "t[np.argmax(prediction)] = 1\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 1650)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack([pattern[0], t]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
