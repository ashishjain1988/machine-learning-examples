{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "import dill\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from absl import flags\n",
    "from keras import regularizers\n",
    "from keras.layers import Activation, BatchNormalization, Convolution1D, Dense, Flatten, GlobalMaxPooling1D, Input,MaxPooling1D, Reshape\n",
    "from keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "from keras.metrics import binary_accuracy, categorical_accuracy\n",
    "from tensorflow.python.ops.nn_ops import conv1d_transpose\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from evolutron.engine import Model, load_model\n",
    "from evolutron.templates import callback_templates as cb\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Tensorflow Classification Model\n",
    "def protein_classification_model(input_shape=None, output_dim=None, saved_model=None):\n",
    "    if saved_model:\n",
    "        model = load_model(saved_model, custom_objects=custom_layers, compile=False)\n",
    "        model.classification = True\n",
    "    else:\n",
    "        seq_length, alphabet = input_shape\n",
    "        # Model Architecture\n",
    "        # Input LayerRO\n",
    "        inp = Input(shape=input_shape, name='aa_seq')\n",
    "        feature_layer = inp\n",
    "        feature_layer = Convolution1D(filters=128,\n",
    "                                          kernel_size=50,\n",
    "                                          strides=1,\n",
    "                                          padding='same',\n",
    "                                          use_bias=False,\n",
    "                                          kernel_initializer='glorot_uniform',\n",
    "                                          activation='linear',\n",
    "                                          name=\"Conv_Layer_1\")(feature_layer)\n",
    "        feature_layer = BatchNormalization()(feature_layer)\n",
    "        feature_layer = Activation(activation='relu')(feature_layer)\n",
    "\n",
    "        # Max-pooling\n",
    "        if seq_length:\n",
    "            max_pool = MaxPooling1D(pool_size=seq_length)(feature_layer)\n",
    "            flat = Flatten()(max_pool)\n",
    "        else:\n",
    "            # max_pool = GlobalMaxPooling1D()(convs[-1])\n",
    "            # flat = max_pool\n",
    "            raise NotImplementedError('Sequence length must be known at this point. Pad and use mask.')\n",
    "\n",
    "        # Fully-Connected encoding layers\n",
    "        fc_enc = Dense(128,\n",
    "                        kernel_initializer='glorot_uniform',\n",
    "                        activation='relu',\n",
    "                        name='FCEnc1')(flat)\n",
    "\n",
    "        encoded = Dense(128,\n",
    "                        kernel_initializer='glorot_uniform',\n",
    "                        activation='relu',\n",
    "                        name='FCEnc2')(fc_enc)\n",
    "\n",
    "        classifier = Dense(output_dim,\n",
    "                           kernel_initializer='glorot_uniform',\n",
    "                           activation='softmax',\n",
    "                           name='Classifier')(encoded)\n",
    "\n",
    "        model = Model(inputs=inp, outputs=classifier, name='CoFAM', classification=True)\n",
    "\n",
    "    # Loss Functions\n",
    "    losses = [categorical_crossentropy]\n",
    "\n",
    "    # Metrics\n",
    "    metrics = [categorical_accuracy]\n",
    "\n",
    "    # Compilation\n",
    "    model.compile(optimizer=\"nadam\",\n",
    "                  loss=losses,\n",
    "                  metrics=metrics,\n",
    "                  lr=0.002)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Functions for Padding of the dataset\n",
    "def pad_or_clip_seq(x, n):\n",
    "    if n >= x.shape[0]:\n",
    "        b = np.zeros((n, x.shape[1]))\n",
    "        b[:x.shape[0]] = x\n",
    "        return b\n",
    "    else:\n",
    "        return x[:n, :]\n",
    "    \n",
    "def preprocess_dataset(x_data, y_data=None, one_hot='x', padded=True, pad_y_data=False, nb_aa=20, min_aa=None,\n",
    "                       max_aa=None):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        x_data (pd.Series):\n",
    "        y_data (list or np.ndArray):\n",
    "        one_hot (str):\n",
    "        padded (bool):\n",
    "        pad_y_data (bool):\n",
    "        nb_aa:\n",
    "        min_aa:\n",
    "        max_aa:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    if not max_aa:\n",
    "        max_aa = int(np.percentile([len(x) for x in x_data], 99))  # pad so that 99% of datapoints are complete\n",
    "    else:\n",
    "        max_aa = min(max_aa, np.max([len(x) for x in x_data]))\n",
    "    x_data = np.asarray([pad_or_clip_seq(x, max_aa) for x in x_data], dtype=np.float32)\n",
    "\n",
    "    y_data = np.asarray(y_data)\n",
    "    assert ((len(x_data) == len(y_data)) or (len(x_data) == len(y_data[0])))\n",
    "    data_size = len(x_data)\n",
    "    print('Dataset size: {0}'.format(data_size))\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data,y_data=preprocess_dataset(filteredEncodingTensors,y_data,max_aa=1353)\n",
    "input_shape = x_data[0].shape\n",
    "y_data = to_categorical(y_data)\n",
    "output_dim = y_data.shape[1]\n",
    "conv_net = protein_classification_model(input_shape,output_dim)\n",
    "plot_model(conv_net, to_file='model.png')\n",
    "conv_net.display_network_info()\n",
    "callbacks = cb.standard(patience=20, reduce_factor=0.5)\n",
    "#print('Started training at {}'.format(time.asctime()))\n",
    "conv_net.fit(x_data, y_data,epochs=10,batch_size=64,validation_split=0.15,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Save Model\n",
    "outputFolder=\"output\"\n",
    "file_key = str(np.random.randint(10 ** 8, 10 ** 9))\n",
    "conv_net.save_train_history(file_key, data_dir=outputFolder)\n",
    "conv_net.save(file_key, data_dir=outputFolder)\n",
    "conv_net.save_architecture(file_key, data_dir=outputFolder)\n",
    "dill.dump(FLAGS.flag_values_dict(),open(os.path.join(outputFolder, 'models', file_key + '.flags'), 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:allennlp]",
   "language": "python",
   "name": "conda-env-allennlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
