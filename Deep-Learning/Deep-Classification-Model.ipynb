{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/efs/home/ctg3039/.conda/envs/allennlp/lib/python3.6/site-packages/sklearn/utils/linear_assignment_.py:21: DeprecationWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "import dill\n",
    "import numpy as np\n",
    "from matplotlib.ticker import NullFormatter\n",
    "from sklearn import manifold, datasets\n",
    "import s3fs\n",
    "import csv\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchviz as viz\n",
    "model_dir = Path('.')\n",
    "weights = model_dir / 'weights.hdf5'\n",
    "options = model_dir / 'options.json'\n",
    "#For using GPU\n",
    "seqvec  = ElmoEmbedder(options,weights,cuda_device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.make_dot(seqvec.elmo_bilm.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Get the protein sequence\n",
    "raw_data = pd.read_csv('../CoMET/example/sprot_dna_tf_pfam.tsv', sep=\"\\t\", header='infer')\n",
    "raw_data.columns = raw_data.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "length = [len(x) for x in raw_data.sequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the embeddings of the TFs\n",
    "kernelList = dill.load(open('TF-SeqVec-Embeddings.pickle','rb'))\n",
    "kList=[]\n",
    "for entry in raw_data.entry:\n",
    "    kList.append(kernelList[entry].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/efs/home/ctg3039/.conda/envs/allennlp/lib/python3.6/site-packages/ipykernel_launcher.py:49: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    }
   ],
   "source": [
    "def csv_parser(filename, codes=False, code_key=None, sep='\\t'):\n",
    "    def fam(x):\n",
    "        dt = str(x).split(',')\n",
    "        f = [d for d in dt if d.find(' family') >= 0]\n",
    "\n",
    "        try:\n",
    "            return f[0]\n",
    "        except IndexError:\n",
    "            return \"Unassigned\"\n",
    "\n",
    "    def supfam(x):\n",
    "        dt = str(x).split(',')\n",
    "        f = [d for d in dt if d.find('superfamily') >= 0]\n",
    "\n",
    "        try:\n",
    "            return f[0]\n",
    "        except IndexError:\n",
    "            return \"Unassigned\"\n",
    "\n",
    "    def subfam(x):\n",
    "        dt = str(x).split(',')\n",
    "        f = [d for d in dt if d.find('subfamily') >= 0]\n",
    "\n",
    "        try:\n",
    "            return f[0]\n",
    "        except IndexError:\n",
    "            return \"Unassigned\"\n",
    "\n",
    "    try:\n",
    "        raw_data = pd.read_hdf(filename.split('.')[0] + '.h5', 'raw_data')\n",
    "    except FileNotFoundError:\n",
    "        raw_data = pd.read_csv(filename, sep=sep, header='infer')\n",
    "        raw_data.columns = raw_data.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "        try:\n",
    "            raw_data['fam'] = raw_data['protein_families'].apply(fam)\n",
    "            raw_data['sup'] = raw_data['protein_families'].apply(supfam)\n",
    "            raw_data['sub'] = raw_data['protein_families'].apply(subfam)\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "        raw_data.to_hdf(filename.split('.')[0] + '.h5', 'raw_data')\n",
    "        \n",
    "    ##For subsetting the data\n",
    "    raw_data = raw_data.head(n=2000)\n",
    "    if codes:\n",
    "        if type(code_key) == str:\n",
    "            code_vc = raw_data[code_key].value_counts()\n",
    "            pos_data = raw_data[raw_data[code_key] != 'Unassigned'][\n",
    "                ~raw_data[code_key].isin(code_vc[code_vc == 1].index.tolist())]\n",
    "            code_cats = pos_data[code_key].astype('category').cat.codes\n",
    "            y_data = code_cats.tolist()\n",
    "            y_data = [y + 1 for y in y_data]\n",
    "            x_data = pos_data.sequence\n",
    "            names = pos_data.entry\n",
    "        else:\n",
    "            pf = raw_data[code_key]\n",
    "            pos_data = raw_data[pf[code_key[0]].notnull() & pf[code_key[1]].notnull()]\n",
    "            y_data = [pos_data[k].tolist() for k in code_key]\n",
    "            x_data = pos_data.sequence\n",
    "            names = pos_data.entry\n",
    "    else:\n",
    "        x_data = raw_data.sequence\n",
    "        y_data = None\n",
    "\n",
    "    return x_data, y_data, names\n",
    "\n",
    "x_data, y_data, proteinNames = csv_parser(\"../CoMET/example/sprot_dna_tf_pfam.tsv\", codes=True, code_key=\"fam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.DataFrame(zip(proteinNames,y_data))\n",
    "with open(\"TFproteinsWithFamilies.txt\",\"w\") as handle:\n",
    "    #writer = csv.writer(handle, delimiter='\\t')\n",
    "    for index, row in d.iterrows():\n",
    "        handle.write(row[0]+\"\\t\"+str(row[1]))\n",
    "        handle.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodingTensors = dill.load(open('TF-SeqVec-Protein-Embeddings.pickle','rb'))\n",
    "#encodingTensors = dill.load(open('TF-SeqVec-Embeddings.pickle','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredEncodingTensors = []\n",
    "for protein in proteinNames:\n",
    "    filteredEncodingTensors.append(encodingTensors[protein].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump(filteredEncodingTensors1,open('TF-SeqVec-Protein-WithFamilies-Embeddings.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "a = torch.randn(32, 100, 1)  \n",
    "m = nn.Conv1d(100, 100, 1) \n",
    "out = m(a)\n",
    "print(out.size())\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, TensorDataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "x_tensor = torch.stack(filteredEncodingTensors)\n",
    "#x_tensor = filteredEncodingTensors\n",
    "y_tensor = torch.from_numpy(np.array(y_data)).float()\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        self.x = x_tensor\n",
    "        self.y = y_tensor\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "# Builds dataset with ALL data\n",
    "dataset = CustomDataset(x_tensor, y_tensor)\n",
    "\n",
    "# Splits randomly into train and validation datasets\n",
    "train_dataset, val_dataset = random_split(dataset, [5615, 992])\n",
    "\n",
    "# Builds a loader for each dataset to perform mini-batch gradient descent\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=20)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=20)\n",
    "\n",
    "model1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 128, kernel_size=5,stride=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            #nn.MaxPool1d(kernel_size=pool_kernel_size, stride=pool_kernel_size),\n",
    "            nn.Dropout(p=0.2),\n",
    "\n",
    "            #nn.Conv1d(128, 256, kernel_size=5,stride=1),\n",
    "            #nn.ReLU(inplace=True),\n",
    "            #nn.MaxPool1d(kernel_size=pool_kernel_size, stride=pool_kernel_size),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(128,256),\n",
    "            #nn.ReLU(inplace=True),\n",
    "            nn.Linear(256,240),\n",
    "            nn.Sigmoid()).to(device)\n",
    "print(model1)\n",
    "\n",
    "# Sets hyper-parameters\n",
    "lr = 1e-1\n",
    "n_epochs = 10\n",
    "\n",
    "# Defines loss function and optimizer\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model1.parameters(), lr=lr)\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Creates function to perform train step from model, loss and optimizer\n",
    "def make_train_step(model, loss_fn, optimizer):\n",
    "    # Builds function that performs a step in the train loop\n",
    "    def train_step(x, y):\n",
    "        # Sets model to TRAIN mode\n",
    "        model.train()\n",
    "        # Makes predictions\n",
    "        yhat = model(x)\n",
    "        # Computes loss\n",
    "        loss = loss_fn(y, yhat)\n",
    "        # Computes gradients\n",
    "        loss.backward()\n",
    "        # Updates parameters and zeroes gradients\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "    \n",
    "    # Returns the function that will be called inside the train loop\n",
    "    return train_step\n",
    "train_step = make_train_step(model1, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # Uses loader to fetch one mini-batch for training\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # NOW, sends the mini-batch data to the device\n",
    "        # so it matches location of the MODEL\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        # One stpe of training\n",
    "        loss = train_step(x_batch, y_batch)\n",
    "        losses.append(loss)\n",
    "        \n",
    "    # After finishing training steps for all mini-batches,\n",
    "    # it is time for evaluation!\n",
    "        \n",
    "    # We tell PyTorch to NOT use autograd...\n",
    "    # Do you remember why?\n",
    "    with torch.no_grad():\n",
    "        # Uses loader to fetch one mini-batch for validation\n",
    "        for x_val, y_val in val_loader:\n",
    "            # Again, sends data to same device as model\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "            \n",
    "            # What is that?!\n",
    "            model.eval()\n",
    "            # Makes predictions\n",
    "            yhat = model(x_val)\n",
    "            # Computes validation loss\n",
    "            val_loss = loss_fn(y_val, yhat)\n",
    "            val_losses.append(val_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "#x_tensor = torch.from_numpy(x).float()\n",
    "x_tensor = filteredEncodingTensors\n",
    "y_tensor = torch.from_numpy(y_data).float()\n",
    "\n",
    "# Builds dataset with ALL data\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "# Splits randomly into train and validation datasets\n",
    "train_dataset, val_dataset = random_split(dataset, [80, 20])\n",
    "\n",
    "# Builds a loader for each dataset to perform mini-batch gradient descent\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=20)\n",
    "\n",
    "# Builds a simple sequential model\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "print(model.state_dict())\n",
    "model1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 128, kernel_size=conv_kernel_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            #nn.MaxPool1d(kernel_size=pool_kernel_size, stride=pool_kernel_size),\n",
    "            nn.Dropout(p=0.2),\n",
    "\n",
    "            nn.Conv1d(128, 256, kernel_size=conv_kernel_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            #nn.MaxPool1d(kernel_size=pool_kernel_size, stride=pool_kernel_size),\n",
    "            nn.Dropout(p=0.2),\n",
    "            #nn.Linear(960 * self.n_channels, n_genomic_features),\n",
    "            #nn.ReLU(inplace=True),\n",
    "            nn.Linear(256,50),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "\n",
    "# Sets hyper-parameters\n",
    "lr = 1e-1\n",
    "n_epochs = 150\n",
    "\n",
    "# Defines loss function and optimizer\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Creates function to perform train step from model, loss and optimizer\n",
    "def make_train_step(model, loss_fn, optimizer):\n",
    "    # Builds function that performs a step in the train loop\n",
    "    def train_step(x, y):\n",
    "        # Sets model to TRAIN mode\n",
    "        model.train()\n",
    "        # Makes predictions\n",
    "        yhat = model(x)\n",
    "        # Computes loss\n",
    "        loss = loss_fn(y, yhat)\n",
    "        # Computes gradients\n",
    "        loss.backward()\n",
    "        # Updates parameters and zeroes gradients\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "    \n",
    "    # Returns the function that will be called inside the train loop\n",
    "    return train_step\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # Uses loader to fetch one mini-batch for training\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # NOW, sends the mini-batch data to the device\n",
    "        # so it matches location of the MODEL\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        # One stpe of training\n",
    "        loss = train_step(x_batch, y_batch)\n",
    "        losses.append(loss)\n",
    "        \n",
    "    # After finishing training steps for all mini-batches,\n",
    "    # it is time for evaluation!\n",
    "        \n",
    "    # We tell PyTorch to NOT use autograd...\n",
    "    # Do you remember why?\n",
    "    with torch.no_grad():\n",
    "        # Uses loader to fetch one mini-batch for validation\n",
    "        for x_val, y_val in val_loader:\n",
    "            # Again, sends data to same device as model\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "            \n",
    "            # What is that?!\n",
    "            model.eval()\n",
    "            # Makes predictions\n",
    "            yhat = model(x_val)\n",
    "            # Computes validation loss\n",
    "            val_loss = loss_fn(y_val, yhat)\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "print(model.state_dict())\n",
    "print(np.mean(losses))\n",
    "print(np.mean(val_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from absl import flags\n",
    "from keras import regularizers\n",
    "from keras.layers import Activation, BatchNormalization, Convolution1D, Dense, Flatten, GlobalMaxPooling1D, Input, \\\n",
    "    MaxPooling1D, Reshape\n",
    "from keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "from keras.metrics import binary_accuracy, categorical_accuracy\n",
    "from tensorflow.python.ops.nn_ops import conv1d_transpose\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from evolutron.engine import Model, load_model\n",
    "from evolutron.templates import callback_templates as cb\n",
    "\n",
    "##Tensorflow Model\n",
    "def build_cofam_model(input_shape=None, output_dim=None, saved_model=None):\n",
    "    if saved_model:\n",
    "        model = load_model(saved_model, custom_objects=custom_layers, compile=False)\n",
    "        model.classification = True\n",
    "    else:\n",
    "        seq_length, alphabet = input_shape\n",
    "        # Model Architecture\n",
    "        # Input LayerRO\n",
    "        inp = Input(shape=input_shape, name='ProteinEmbeddings')\n",
    "        feature_layer = inp\n",
    "        feature_layer = Convolution1D(filters=128,\n",
    "                                          kernel_size=50,\n",
    "                                          strides=1,\n",
    "                                          padding='same',\n",
    "                                          use_bias=False,\n",
    "                                          kernel_initializer='glorot_uniform',\n",
    "                                          activation='linear',\n",
    "                                          name=\"ConvLayer1\")(feature_layer)\n",
    "        #feature_layer = BatchNormalization()(feature_layer)\n",
    "        feature_layer = Activation(activation='relu',name=\"ActivationLayer1\")(feature_layer)\n",
    "        \n",
    "        feature_layer = Convolution1D(filters=256,\n",
    "                                          kernel_size=50,\n",
    "                                          strides=1,\n",
    "                                          padding='same',\n",
    "                                          use_bias=False,\n",
    "                                          kernel_initializer='glorot_uniform',\n",
    "                                          activation='linear',\n",
    "                                          name=\"ConvLayer2\")(feature_layer)\n",
    "        #feature_layer = BatchNormalization()(feature_layer)\n",
    "        feature_layer = Activation(activation='relu',name=\"ActivationLayer2\")(feature_layer)\n",
    "\n",
    "        # Max-pooling\n",
    "        if seq_length:\n",
    "            max_pool = MaxPooling1D(pool_size=seq_length,name=\"Max-Pooling\")(feature_layer)\n",
    "            flat = Flatten(name=\"Flatten\")(max_pool)\n",
    "        else:\n",
    "            # max_pool = GlobalMaxPooling1D()(convs[-1])\n",
    "            # flat = max_pool\n",
    "            raise NotImplementedError('Sequence length must be known at this point. Pad and use mask.')\n",
    "\n",
    "        # Fully-Connected encoding layers\n",
    "        fc_enc = Dense(256,\n",
    "                        kernel_initializer='glorot_uniform',\n",
    "                        activation='relu',\n",
    "                        name='FullyConnected1')(flat)\n",
    "\n",
    "        encoded = Dense(256,\n",
    "                        kernel_initializer='glorot_uniform',\n",
    "                        activation='relu',\n",
    "                        name='FullyConnected2')(fc_enc)\n",
    "\n",
    "        classifier = Dense(output_dim,\n",
    "                           kernel_initializer='glorot_uniform',\n",
    "                           activation='softmax',\n",
    "                           name='Classifier')(encoded)\n",
    "\n",
    "        model = Model(inputs=inp, outputs=classifier, name='DeepClassfier', classification=True)\n",
    "\n",
    "    # Loss Functions\n",
    "    losses = [categorical_crossentropy]\n",
    "\n",
    "    # Metrics\n",
    "    metrics = [categorical_accuracy]\n",
    "\n",
    "    # Compilation\n",
    "\n",
    "    model.compile(optimizer=\"nadam\",\n",
    "                  loss=losses,\n",
    "                  metrics=metrics,\n",
    "                  lr=0.002)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredEncodingTensors = dill.load(open('TF-SeqVec-Protein-WithFamiliesTop2000-Embeddings.pickle','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredEncodingTensors = []\n",
    "for protein in proteinNames:\n",
    "    filteredEncodingTensors.append(encodingTensors[protein].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Padding of the dataset\n",
    "def pad_or_clip_seq(x, n):\n",
    "    if n >= x.shape[0]:\n",
    "        b = np.zeros((n, x.shape[1]))\n",
    "        b[:x.shape[0]] = x\n",
    "        return b\n",
    "    else:\n",
    "        return x[:n, :]\n",
    "    \n",
    "def preprocess_dataset(x_data, y_data=None, one_hot='x', padded=True, pad_y_data=False, nb_aa=20, min_aa=None,\n",
    "                       max_aa=None):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        x_data (pd.Series):\n",
    "        y_data (list or np.ndArray):\n",
    "        one_hot (str):\n",
    "        padded (bool):\n",
    "        pad_y_data (bool):\n",
    "        nb_aa:\n",
    "        min_aa:\n",
    "        max_aa:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    if not max_aa:\n",
    "        max_aa = int(np.percentile([len(x) for x in x_data], 99))  # pad so that 99% of datapoints are complete\n",
    "    else:\n",
    "        max_aa = min(max_aa, np.max([len(x) for x in x_data]))\n",
    "    x_data = np.asarray([pad_or_clip_seq(x, max_aa) for x in x_data], dtype=np.float32)\n",
    "\n",
    "    y_data = np.asarray(y_data)\n",
    "    assert ((len(x_data) == len(y_data)) or (len(x_data) == len(y_data[0])))\n",
    "    data_size = len(x_data)\n",
    "    print('Dataset size: {0}'.format(data_size))\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1094\n"
     ]
    }
   ],
   "source": [
    "x_data,y_data=preprocess_dataset(filteredEncodingTensors,y_data,max_aa=1353)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                                 Output Shape                            Param #        \n",
      "====================================================================================================\n",
      "ProteinEmbeddings (InputLayer)               (None, 1353, 1024)                      0              \n",
      "____________________________________________________________________________________________________\n",
      "ConvLayer1 (Conv1D)                          (None, 1353, 128)                       6553600        \n",
      "____________________________________________________________________________________________________\n",
      "ActivationLayer1 (Activation)                (None, 1353, 128)                       0              \n",
      "____________________________________________________________________________________________________\n",
      "ConvLayer2 (Conv1D)                          (None, 1353, 256)                       1638400        \n",
      "____________________________________________________________________________________________________\n",
      "ActivationLayer2 (Activation)                (None, 1353, 256)                       0              \n",
      "____________________________________________________________________________________________________\n",
      "Max-Pooling (MaxPooling1D)                   (None, 1, 256)                          0              \n",
      "____________________________________________________________________________________________________\n",
      "Flatten (Flatten)                            (None, 256)                             0              \n",
      "____________________________________________________________________________________________________\n",
      "FullyConnected1 (Dense)                      (None, 256)                             65792          \n",
      "____________________________________________________________________________________________________\n",
      "FullyConnected2 (Dense)                      (None, 256)                             65792          \n",
      "____________________________________________________________________________________________________\n",
      "Classifier (Dense)                           (None, 63)                              16191          \n",
      "====================================================================================================\n",
      "Total params: 8,339,775\n",
      "Trainable params: 8,339,775\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#x_data,y_data=preprocess_dataset(filteredEncodingTensors,y_data)\n",
    "\n",
    "input_shape = x_data[0].shape\n",
    "y_data = to_categorical(y_data)\n",
    "output_dim = y_data.shape[1]\n",
    "conv_net = build_cofam_model(input_shape,output_dim)\n",
    "conv_net.display_network_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = cb.standard(patience=20, reduce_factor=0.5)\n",
    "#print('Started training at {}'.format(time.asctime()))\n",
    "conv_net.fit(x_data, y_data,epochs=10,batch_size=64,validation_split=0.15,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(conv_net, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:allennlp]",
   "language": "python",
   "name": "conda-env-allennlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
